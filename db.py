# db.py (Supabase-only) — UPDATED for forecasts_daily + forecast_extras_hourly (+ legacy fallback)
from __future__ import annotations

import os
import json
from typing import Any, Dict, List, Optional, Tuple

from etl_utils import utc_now_z

# -------------------------
# Connection / init
# -------------------------

def _db_url() -> str:
    url = os.getenv("WEATHER_DB_URL") or os.getenv("DATABASE_URL")
    if not url:
        raise RuntimeError("Missing WEATHER_DB_URL (or DATABASE_URL) for Supabase Postgres.")
    return url


def get_conn():
    url = _db_url()
    try:
        import psycopg  # type: ignore
        return psycopg.connect(url)
    except ImportError:
        import psycopg2  # type: ignore
        return psycopg2.connect(url)


def init_db() -> None:
    conn = get_conn()
    conn.close()


def _has_table(conn, fq_table: str) -> bool:
    # fq_table like "public.forecasts_daily"
    with conn.cursor() as cur:
        cur.execute("select to_regclass(%s) is not null", (fq_table,))
        return bool(cur.fetchone()[0])


# -------------------------
# Locations
# -------------------------

def upsert_location(station: dict) -> None:
    conn = get_conn()
    cur = conn.cursor()

    cur.execute(
        """
        insert into public.locations (station_id, name, state, timezone, lat, lon, elevation_ft, is_active)
        values (%s,%s,%s,%s,%s,%s,%s,%s)
        on conflict (station_id) do update set
          name=coalesce(excluded.name, public.locations.name),
          state=coalesce(excluded.state, public.locations.state),
          timezone=coalesce(excluded.timezone, public.locations.timezone),
          lat=coalesce(excluded.lat, public.locations.lat),
          lon=coalesce(excluded.lon, public.locations.lon),
          elevation_ft=coalesce(excluded.elevation_ft, public.locations.elevation_ft),
          is_active=coalesce(excluded.is_active, public.locations.is_active)
        """,
        (
            station["station_id"],
            station.get("name"),
            station.get("state"),
            station.get("timezone"),
            station.get("lat"),
            station.get("lon"),
            station.get("elevation_ft"),
            station.get("is_active", True),
        ),
    )

    conn.commit()
    conn.close()


# -------------------------
# Observations (supports multi-run snapshots IF schema exists)
# -------------------------

_OBS_SOURCE_PRIORITY = {
    "NWS_CLI": 100,
    "nws_cli": 100,
    "NWS_OBS_FALLBACK": 10,
    "nws_station_obs": 10,
}

def _obs_priority(source: str) -> int:
    return _OBS_SOURCE_PRIORITY.get(source, 0)

_OBS_RUN_ISSUED_AT: Optional[str] = None

def _get_obs_run_issued_at() -> str:
    global _OBS_RUN_ISSUED_AT
    if _OBS_RUN_ISSUED_AT is None:
        _OBS_RUN_ISSUED_AT = utc_now_z()
    return _OBS_RUN_ISSUED_AT


def get_or_create_observation_run(run_issued_at: str, conn=None) -> Any:
    owns = False
    if conn is None:
        conn = get_conn()
        owns = True

    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                insert into public.observation_runs (run_issued_at)
                values (%s::timestamptz)
                on conflict (run_issued_at) do update set run_issued_at = excluded.run_issued_at
                returning run_id
                """,
                (run_issued_at,),
            )
            run_id = cur.fetchone()[0]
        if owns:
            conn.commit()
        return run_id
    finally:
        if owns:
            conn.close()


def upsert_observation(
    station_id: str,
    obs_date: str,
    observed_high: float,
    observed_low: float,
    issued_at: Optional[str] = None,
    raw_text: Optional[str] = None,
    source: str = "nws_station_obs",
    *,
    run_issued_at: Optional[str] = None,
) -> None:
    conn = get_conn()
    cur = conn.cursor()
    run_issued_at = run_issued_at or _get_obs_run_issued_at()

    try:
        cur.execute("""
            select
              to_regclass('public.observation_runs') is not null as has_runs,
              to_regclass('public.observations_v2') is not null as has_v2
        """)
        has_runs, has_v2 = cur.fetchone()
        has_new = bool(has_runs) and bool(has_v2)

        if has_new:
            run_id = get_or_create_observation_run(run_issued_at, conn=conn)

            cur.execute(
                """
                insert into public.observations_v2
                  (run_id, station_id, date, observed_high, observed_low, issued_at, fetched_at, raw_text, source)
                values
                  (%s, %s, %s::date, %s, %s, %s::timestamptz, now(), %s, %s)
                on conflict (run_id, station_id, date) do update set
                  observed_high = excluded.observed_high,
                  observed_low  = excluded.observed_low,
                  issued_at     = coalesce(excluded.issued_at, public.observations_v2.issued_at),
                  fetched_at    = now(),
                  raw_text      = coalesce(excluded.raw_text, public.observations_v2.raw_text),
                  source        = excluded.source
                """,
                (run_id, station_id, obs_date, observed_high, observed_low, issued_at, raw_text, source),
            )
            conn.commit()
            return

        # Legacy single-row with "no downgrade"
        cur.execute(
            """
            select source
            from public.observations
            where station_id=%s and date=%s::date
            """,
            (station_id, obs_date),
        )
        row = cur.fetchone()
        existing_source = row[0] if row else None

        if existing_source is not None and _obs_priority(source) < _obs_priority(str(existing_source)):
            cur.execute(
                """
                update public.observations
                set fetched_at = now()
                where station_id=%s and date=%s::date
                """,
                (station_id, obs_date),
            )
            conn.commit()
            return

        cur.execute(
            """
            insert into public.observations
              (station_id, date, observed_high, observed_low, issued_at, fetched_at, raw_text, source)
            values (%s,%s::date,%s,%s,%s::timestamptz, now(), %s, %s)
            on conflict (station_id, date) do update set
              observed_high=excluded.observed_high,
              observed_low=excluded.observed_low,
              issued_at=coalesce(excluded.issued_at, public.observations.issued_at),
              fetched_at=now(),
              raw_text=coalesce(excluded.raw_text, public.observations.raw_text),
              source=excluded.source
            """,
            (station_id, obs_date, observed_high, observed_low, issued_at, raw_text, source),
        )

        conn.commit()
    finally:
        conn.close()


# -------------------------
# Forecast runs
# -------------------------

def get_or_create_forecast_run(source: str, issued_at: str, conn=None) -> Any:
    owns = False
    if conn is None:
        conn = get_conn()
        owns = True
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                insert into public.forecast_runs (source, issued_at)
                values (%s, %s)
                on conflict (source, issued_at) do update set source = excluded.source
                returning run_id
                """,
                (source, issued_at),
            )
            run_id = cur.fetchone()[0]
        if owns:
            conn.commit()
        return run_id
    finally:
        if owns:
            conn.close()


# -------------------------
# Daily forecasts (new preferred: public.forecasts_daily)
# -------------------------

def bulk_upsert_forecasts_daily(conn, rows: list[dict]) -> int:
    """
    rows items:
      run_id, station_id, target_date,
      high_f, low_f, lead_high_hours, lead_low_hours
    """
    if not rows:
        return 0

    sql = """
    insert into public.forecasts_daily (
        run_id, station_id, target_date,
        high_f, low_f,
        lead_high_hours, lead_low_hours
    ) values (
        %(run_id)s, %(station_id)s, %(target_date)s,
        %(high_f)s, %(low_f)s,
        %(lead_high_hours)s, %(lead_low_hours)s
    )
    on conflict (run_id, station_id, target_date)
    do update set
        high_f = excluded.high_f,
        low_f  = excluded.low_f,
        lead_high_hours = excluded.lead_high_hours,
        lead_low_hours  = excluded.lead_low_hours;
    """
    with conn.cursor() as cur:
        cur.executemany(sql, rows)
    return len(rows)


# -------------------------
# Hourly extras (ML) — public.forecast_extras_hourly
# -------------------------

def bulk_upsert_forecast_extras_hourly(conn, rows: list[dict]) -> int:
    """
    rows items:
      run_id, station_id, valid_time,
      temperature_f, dewpoint_f, humidity_pct,
      wind_speed_mph, wind_dir_deg, cloud_cover_pct,
      precip_prob_pct, extras (dict or json string)
    """
    if not rows:
        return 0

    for r in rows:
        ex = r.get("extras")
        if isinstance(ex, dict):
            r["extras"] = json.dumps(ex)

    sql = """
    insert into public.forecast_extras_hourly (
        run_id, station_id, valid_time,
        temperature_f, dewpoint_f, humidity_pct,
        wind_speed_mph, wind_dir_deg, cloud_cover_pct,
        precip_prob_pct, extras
    ) values (
        %(run_id)s, %(station_id)s, %(valid_time)s,
        %(temperature_f)s, %(dewpoint_f)s, %(humidity_pct)s,
        %(wind_speed_mph)s, %(wind_dir_deg)s, %(cloud_cover_pct)s,
        %(precip_prob_pct)s, %(extras)s::jsonb
    )
    on conflict (run_id, station_id, valid_time)
    do update set
        temperature_f = excluded.temperature_f,
        dewpoint_f    = excluded.dewpoint_f,
        humidity_pct  = excluded.humidity_pct,
        wind_speed_mph = excluded.wind_speed_mph,
        wind_dir_deg   = excluded.wind_dir_deg,
        cloud_cover_pct = excluded.cloud_cover_pct,
        precip_prob_pct = excluded.precip_prob_pct,
        extras = excluded.extras;
    """
    with conn.cursor() as cur:
        cur.executemany(sql, rows)
    return len(rows)


# -------------------------
# Legacy forecasts (public.forecasts) — keep for compatibility
# -------------------------

def bulk_upsert_forecast_values(conn, rows: list[dict]) -> int:
    """
    Back-compat writer for the legacy public.forecasts table.
    If the table does not have jsonb 'extras', we omit it automatically.
    """
    if not rows:
        return 0

    has_extras = _has_table(conn, "public.forecasts")  # table exists
    # Detect column existence
    with conn.cursor() as cur:
        cur.execute("""
            select exists (
              select 1 from information_schema.columns
              where table_schema='public' and table_name='forecasts' and column_name='extras'
            )
        """)
        has_extras_col = bool(cur.fetchone()[0])

    if has_extras_col:
        sql = """
        insert into public.forecasts (
            run_id, station_id, target_date, kind, value_f, lead_hours,
            dewpoint_f, humidity_pct, wind_speed_mph, wind_dir_deg,
            cloud_cover_pct, precip_prob_pct, extras
        ) values (
            %(run_id)s, %(station_id)s, %(target_date)s, %(kind)s, %(value_f)s, %(lead_hours)s,
            %(dewpoint_f)s, %(humidity_pct)s, %(wind_speed_mph)s, %(wind_dir_deg)s,
            %(cloud_cover_pct)s, %(precip_prob_pct)s, %(extras)s::jsonb
        )
        on conflict (run_id, station_id, target_date, kind)
        do update set
            value_f = excluded.value_f,
            lead_hours = excluded.lead_hours,
            dewpoint_f = excluded.dewpoint_f,
            humidity_pct = excluded.humidity_pct,
            wind_speed_mph = excluded.wind_speed_mph,
            wind_dir_deg = excluded.wind_dir_deg,
            cloud_cover_pct = excluded.cloud_cover_pct,
            precip_prob_pct = excluded.precip_prob_pct,
            extras = excluded.extras;
        """
        with conn.cursor() as cur:
            cur.executemany(sql, rows)
        return len(rows)

    sql = """
    insert into public.forecasts (
        run_id, station_id, target_date, kind, value_f, lead_hours,
        dewpoint_f, humidity_pct, wind_speed_mph, wind_dir_deg,
        cloud_cover_pct, precip_prob_pct
    ) values (
        %(run_id)s, %(station_id)s, %(target_date)s, %(kind)s, %(value_f)s, %(lead_hours)s,
        %(dewpoint_f)s, %(humidity_pct)s, %(wind_speed_mph)s, %(wind_dir_deg)s,
        %(cloud_cover_pct)s, %(precip_prob_pct)s
    )
    on conflict (run_id, station_id, target_date, kind)
    do update set
        value_f = excluded.value_f,
        lead_hours = excluded.lead_hours,
        dewpoint_f = excluded.dewpoint_f,
        humidity_pct = excluded.humidity_pct,
        wind_speed_mph = excluded.wind_speed_mph,
        wind_dir_deg = excluded.wind_dir_deg,
        cloud_cover_pct = excluded.cloud_cover_pct,
        precip_prob_pct = excluded.precip_prob_pct;
    """
    with conn.cursor() as cur:
        cur.executemany(sql, rows)
    return len(rows)


def upsert_forecast_value(
    *,
    run_id: Any,
    station_id: str,
    target_date: str,
    kind: str,
    value_f: float,
    lead_hours: Optional[float],
    extras: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Legacy row-by-row upsert into public.forecasts (two-row high/low format).
    Prefer bulk_upsert_forecasts_daily for new pipeline.
    """
    extras = extras or {}

    cur_vals = (
        run_id,
        station_id,
        target_date,
        kind,
        value_f,
        lead_hours,
        extras.get("dewpoint_f"),
        extras.get("humidity_pct"),
        extras.get("wind_speed_mph"),
        extras.get("wind_dir_deg"),
        extras.get("cloud_cover_pct"),
        extras.get("precip_prob_pct"),
    )

    conn = get_conn()
    cur = conn.cursor()

    cur.execute(
        """
        insert into public.forecasts
          (run_id, station_id, target_date, kind, value_f, lead_hours,
           dewpoint_f, humidity_pct, wind_speed_mph, wind_dir_deg, cloud_cover_pct, precip_prob_pct)
        values
          (%s,%s,%s::date,%s,%s,%s,%s,%s,%s,%s,%s,%s)
        on conflict (run_id, station_id, target_date, kind) do update set
          value_f=excluded.value_f,
          lead_hours=coalesce(excluded.lead_hours, public.forecasts.lead_hours),
          dewpoint_f=coalesce(excluded.dewpoint_f, public.forecasts.dewpoint_f),
          humidity_pct=coalesce(excluded.humidity_pct, public.forecasts.humidity_pct),
          wind_speed_mph=coalesce(excluded.wind_speed_mph, public.forecasts.wind_speed_mph),
          wind_dir_deg=coalesce(excluded.wind_dir_deg, public.forecasts.wind_dir_deg),
          cloud_cover_pct=coalesce(excluded.cloud_cover_pct, public.forecasts.cloud_cover_pct),
          precip_prob_pct=coalesce(excluded.precip_prob_pct, public.forecasts.precip_prob_pct)
        """,
        cur_vals,
    )

    conn.commit()
    conn.close()


# -------------------------
# Errors + stats (updated to prefer forecasts_daily)
# -------------------------

def build_errors_for_date(target_date: str) -> int:
    conn = get_conn()
    cur = conn.cursor()

    cur.execute("select to_regclass('public.observations_latest') is not null")
    has_latest = bool(cur.fetchone()[0])

    if has_latest:
        cur.execute(
            """
            select station_id, observed_high, observed_low
            from public.observations_latest
            where date=%s::date
            """,
            (target_date,),
        )
    else:
        cur.execute(
            """
            select station_id, observed_high, observed_low
            from public.observations
            where date=%s::date
            """,
            (target_date,),
        )

    obs_rows = cur.fetchall()
    if not obs_rows:
        conn.close()
        return 0

    has_daily = _has_table(conn, "public.forecasts_daily")

    wrote = 0
    for station_id, oh, ol in obs_rows:
        if has_daily:
            cur.execute(
                """
                select d.run_id, r.source, r.issued_at,
                       d.high_f, d.low_f,
                       d.lead_high_hours, d.lead_low_hours
                from public.forecasts_daily d
                join public.forecast_runs r on r.run_id = d.run_id
                where d.station_id=%s and d.target_date=%s::date
                """,
                (station_id, target_date),
            )

            for run_id, source, issued_at, high_f, low_f, lead_high, lead_low in cur.fetchall():
                # high
                if high_f is not None:
                    forecast_id = f"{run_id}|{station_id}|{target_date}|high"
                    observed_f = float(oh)
                    error_f = float(high_f) - observed_f
                    cur.execute(
                        """
                        insert into public.forecast_errors
                          (forecast_id, station_id, source, target_date, kind, issued_at, lead_hours,
                           forecast_f, observed_f, error_f, abs_error_f)
                        values
                          (%s,%s,%s,%s::date,'high',%s,%s,%s,%s,%s,%s)
                        on conflict (forecast_id) do nothing
                        """,
                        (
                            forecast_id,
                            station_id,
                            source,
                            target_date,
                            issued_at,
                            lead_high,
                            high_f,
                            observed_f,
                            error_f,
                            abs(error_f),
                        ),
                    )
                    wrote += 1

                # low
                if low_f is not None:
                    forecast_id = f"{run_id}|{station_id}|{target_date}|low"
                    observed_f = float(ol)
                    error_f = float(low_f) - observed_f
                    cur.execute(
                        """
                        insert into public.forecast_errors
                          (forecast_id, station_id, source, target_date, kind, issued_at, lead_hours,
                           forecast_f, observed_f, error_f, abs_error_f)
                        values
                          (%s,%s,%s,%s::date,'low',%s,%s,%s,%s,%s,%s)
                        on conflict (forecast_id) do nothing
                        """,
                        (
                            forecast_id,
                            station_id,
                            source,
                            target_date,
                            issued_at,
                            lead_low,
                            low_f,
                            observed_f,
                            error_f,
                            abs(error_f),
                        ),
                    )
                    wrote += 1

        else:
            # legacy path
            cur.execute(
                """
                select f.run_id, r.source, r.issued_at, f.kind, f.value_f, f.lead_hours
                from public.forecasts f
                join public.forecast_runs r on r.run_id = f.run_id
                where f.station_id=%s and f.target_date=%s::date
                """,
                (station_id, target_date),
            )

            for run_id, source, issued_at, kind, forecast_f, lead_hours in cur.fetchall():
                if forecast_f is None:
                    continue
                observed_f = float(oh) if kind == "high" else float(ol)
                error_f = float(forecast_f) - observed_f
                abs_error_f = abs(error_f)
                forecast_id = f"{run_id}|{station_id}|{target_date}|{kind}"

                cur.execute(
                    """
                    insert into public.forecast_errors
                      (forecast_id, station_id, source, target_date, kind, issued_at, lead_hours,
                       forecast_f, observed_f, error_f, abs_error_f)
                    values
                      (%s,%s,%s,%s::date,%s,%s,%s,%s,%s,%s,%s)
                    on conflict (forecast_id) do nothing
                    """,
                    (
                        forecast_id,
                        station_id,
                        source,
                        target_date,
                        kind,
                        issued_at,
                        lead_hours,
                        forecast_f,
                        observed_f,
                        error_f,
                        abs_error_f,
                    ),
                )
                wrote += 1

    conn.commit()
    conn.close()
    return wrote


def _percentile(sorted_vals: List[float], p: float) -> float:
    if not sorted_vals:
        return float("nan")
    if len(sorted_vals) == 1:
        return float(sorted_vals[0])
    k = (len(sorted_vals) - 1) * p
    f = int(k)
    c = min(f + 1, len(sorted_vals) - 1)
    if f == c:
        return float(sorted_vals[f])
    d0 = sorted_vals[f] * (c - k)
    d1 = sorted_vals[c] * (k - f)
    return float(d0 + d1)


def compute_revisions_for_run(run_id: Any) -> int:
    """
    Prefer public.forecasts_daily if present; else legacy public.forecasts.
    Writes to public.forecast_revisions (assumes it exists).
    """
    conn = get_conn()
    cur = conn.cursor()

    has_daily = _has_table(conn, "public.forecasts_daily")

    if has_daily:
        cur.execute(
            """
            select r.source, r.issued_at, d.station_id, d.target_date,
                   d.high_f, d.low_f
            from public.forecasts_daily d
            join public.forecast_runs r on r.run_id = d.run_id
            where d.run_id = %s
            """,
            (run_id,),
        )
        rows = cur.fetchall()
        if not rows:
            conn.close()
            return 0

        wrote = 0
        for source, issued_at, station_id, target_date, high_f, low_f in rows:
            # compute revisions for high/low separately
            for kind, forecast_f in (("high", high_f), ("low", low_f)):
                if forecast_f is None:
                    continue

                cur.execute(
                    """
                    select r2.issued_at,
                           case when %s='high' then d2.high_f else d2.low_f end as prev_value
                    from public.forecasts_daily d2
                    join public.forecast_runs r2 on r2.run_id = d2.run_id
                    where d2.station_id=%s
                      and r2.source=%s
                      and d2.target_date=%s::date
                      and r2.issued_at < %s::timestamptz
                    order by r2.issued_at desc
                    limit 1
                    """,
                    (kind, station_id, source, target_date, issued_at),
                )
                prev = cur.fetchone()
                prev_issued_at = prev[0] if prev else None
                prev_forecast_f = float(prev[1]) if (prev and prev[1] is not None) else None
                delta_f = (float(forecast_f) - prev_forecast_f) if prev_forecast_f is not None else None

                cur.execute(
                    """
                    insert into public.forecast_revisions
                      (station_id, source, kind, target_date, issued_at, forecast_f,
                       prev_issued_at, prev_forecast_f, delta_f)
                    values
                      (%s,%s,%s,%s::date,%s::timestamptz,%s,%s::timestamptz,%s,%s)
                    on conflict (station_id, source, kind, target_date, issued_at) do nothing
                    """,
                    (
                        station_id,
                        source,
                        kind,
                        target_date,
                        issued_at,
                        float(forecast_f),
                        prev_issued_at,
                        prev_forecast_f,
                        delta_f,
                    ),
                )
                wrote += 1

        conn.commit()
        conn.close()
        return wrote

    # legacy path
    cur.execute(
        """
        select r.source, r.issued_at, f.station_id, f.target_date, f.kind, f.value_f
        from public.forecasts f
        join public.forecast_runs r on r.run_id = f.run_id
        where f.run_id = %s
        """,
        (run_id,),
    )
    rows = cur.fetchall()
    if not rows:
        conn.close()
        return 0

    wrote = 0
    for source, issued_at, station_id, target_date, kind, forecast_f in rows:
        if forecast_f is None:
            continue

        cur.execute(
            """
            select r2.issued_at, f2.value_f
            from public.forecasts f2
            join public.forecast_runs r2 on r2.run_id = f2.run_id
            where f2.station_id=%s
              and r2.source=%s
              and f2.kind=%s
              and f2.target_date=%s::date
              and r2.issued_at < %s::timestamptz
            order by r2.issued_at desc
            limit 1
            """,
            (station_id, source, kind, target_date, issued_at),
        )
        prev = cur.fetchone()
        prev_issued_at = prev[0] if prev else None
        prev_forecast_f = float(prev[1]) if (prev and prev[1] is not None) else None
        delta_f = (float(forecast_f) - prev_forecast_f) if prev_forecast_f is not None else None

        cur.execute(
            """
            insert into public.forecast_revisions
              (station_id, source, kind, target_date, issued_at, forecast_f,
               prev_issued_at, prev_forecast_f, delta_f)
            values
              (%s,%s,%s,%s::date,%s::timestamptz,%s,%s::timestamptz,%s,%s)
            on conflict (station_id, source, kind, target_date, issued_at) do nothing
            """,
            (
                station_id,
                source,
                kind,
                target_date,
                issued_at,
                float(forecast_f),
                prev_issued_at,
                prev_forecast_f,
                delta_f,
            ),
        )
        wrote += 1

    conn.commit()
    conn.close()
    return wrote


def update_error_stats(*, window_days: int, station_id: Optional[str] = None) -> None:
    conn = get_conn()
    cur = conn.cursor()

    params: List[Any] = [window_days]
    station_clause = ""
    if station_id:
        station_clause = "and station_id=%s"
        params.append(station_id)

    cur.execute(
        f"""
        select station_id, source, kind, error_f, abs_error_f
        from public.forecast_errors
        where target_date >= (now()::date - (%s::int * interval '1 day'))
        {station_clause}
        """,
        params,
    )
    rows = cur.fetchall()

    by: Dict[Tuple[str, str, str], List[Tuple[float, float]]] = {}
    for st_id, source, kind, e, ae in rows:
        if st_id is None or source is None or kind is None:
            continue
        if e is None or ae is None:
            continue
        by.setdefault((str(st_id), str(source), str(kind)), []).append((float(e), float(ae)))

    now_ts = utc_now_z()

    for (st_id, source, kind), vals in by.items():
        n = len(vals)
        if n == 0:
            continue

        errors = [v[0] for v in vals]
        abs_errors = [v[1] for v in vals]

        bias = sum(errors) / n
        mae = sum(abs_errors) / n
        rmse = (sum((x * x) for x in errors) / n) ** 0.5

        se = sorted(errors)
        p10 = _percentile(se, 0.10)
        p50 = _percentile(se, 0.50)
        p90 = _percentile(se, 0.90)

        cur.execute(
            """
            insert into public.error_stats
              (station_id, source, kind, window_days, n, bias, mae, rmse, p10, p50, p90, last_updated)
            values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s::timestamptz)
            on conflict (station_id, source, kind, window_days) do update set
              n=excluded.n, bias=excluded.bias, mae=excluded.mae, rmse=excluded.rmse,
              p10=excluded.p10, p50=excluded.p50, p90=excluded.p90, last_updated=excluded.last_updated
            """,
            (st_id, source, kind, window_days, n, bias, mae, rmse, p10, p50, p90, now_ts),
        )

    stations_sources = sorted({(k[0], k[1]) for k in by.keys()})
    for st_id, source in stations_sources:
        highs = by.get((st_id, source, "high"), [])
        lows = by.get((st_id, source, "low"), [])
        if not highs or not lows:
            continue

        mae_high = sum(v[1] for v in highs) / len(highs)
        mae_low = sum(v[1] for v in lows) / len(lows)
        mae_both = (mae_high + mae_low) / 2.0
        n = min(len(highs), len(lows))

        cur.execute(
            """
            insert into public.error_stats
              (station_id, source, kind, window_days, n, mae, last_updated)
            values (%s,%s,'both',%s,%s,%s,%s::timestamptz)
            on conflict (station_id, source, kind, window_days) do update set
              n=excluded.n, mae=excluded.mae, last_updated=excluded.last_updated
            """,
            (st_id, source, window_days, n, mae_both, now_ts),
        )

    conn.commit()
    conn.close()
